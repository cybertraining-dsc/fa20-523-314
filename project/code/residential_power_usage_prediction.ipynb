{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "residential_power_usage_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUUe-L6AvNdx"
      },
      "source": [
        "# Residential Power Usage Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07XDnUUGeCZL"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#! pip install cloudmesh-common -U\n",
        "from cloudmesh.common.StopWatch import StopWatch\n",
        "\n",
        "# Misc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbe-TgkDmrB0"
      },
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMCJGlphxGIy"
      },
      "source": [
        "%cd /content/drive/My Drive/BigData_Project/Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISK_yCHe-QY3"
      },
      "source": [
        "## Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "962rhRQInPK-"
      },
      "source": [
        "#!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUVqxZAq4sPe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14f39b7-caaa-4cae-d037-04d403ecaab9"
      },
      "source": [
        "if not os.path.exists('residential-power-usage-3years-data-timeseries.zip'):\n",
        "  StopWatch.start(\"Data download\")\n",
        "  os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/My Drive/BigData_Project\" # put path for wherever you put it\n",
        "  !mkdir ~/.kaggle\n",
        "  !cp /content/drive/'My Drive'/BigData_Project/kaggle.json ~/.kaggle\n",
        "  !chmod 600 ~/.kaggle/kaggle.json\n",
        "  !kaggle datasets download -d srinuti/residential-power-usage-3years-data-timeseries\n",
        "  StopWatch.stop(\"Data download\")\n",
        "  StopWatch.status(\"Data download\", True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading residential-power-usage-3years-data-timeseries.zip to /content/drive/My Drive/BigData_Project/Data\n",
            "\r  0% 0.00/227k [00:00<?, ?B/s]\n",
            "\r100% 227k/227k [00:00<00:00, 31.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNeY-b2OJb2e"
      },
      "source": [
        "## Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTP9VUHHB8CB"
      },
      "source": [
        "data_path = os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cQlj9yO9AMP"
      },
      "source": [
        "\n",
        "unzippingReq = True #please modify this flag unzippingReq from False to True \n",
        "if unzippingReq: #please modify this code unzippingReq from False to True \n",
        "    StopWatch.start(\"Data load\")\n",
        "    file_name = data_path  + '/residential-power-usage-3years-data-timeseries.zip'\n",
        "    zip_ref = zipfile.ZipFile(file_name, 'r')\n",
        "    zip_ref.extractall(path=data_path)\n",
        "    zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpE3er-zCUPX"
      },
      "source": [
        "if len(os.listdir()) > 1:\n",
        "  power_usage = pd.read_csv('power_usage_2016_to_2020.csv')\n",
        "  weather = pd.read_csv('weather_2016_2020_daily.csv')\n",
        "if unzippingReq:\n",
        "  StopWatch.stop(\"Data load\")\n",
        "  StopWatch.status(\"Data load\", True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmcabqiGDpcg"
      },
      "source": [
        "## Data descriptive analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkLmGwO_Dolv"
      },
      "source": [
        "power_usage.head(25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GZUedIVEHaR"
      },
      "source": [
        "weather.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHxLcaRPEOeE"
      },
      "source": [
        "print(power_usage.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQgZuxwgEOn_"
      },
      "source": [
        "print(weather.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPH_78zcEOpu"
      },
      "source": [
        "power_usage.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogz9Lm5aEOvS"
      },
      "source": [
        "power_usage.notes.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW6dc7JBEloJ"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNIALL_FI9if"
      },
      "source": [
        "### Change column name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uTho5xn1d5M"
      },
      "source": [
        "StopWatch.start(\"Data preprocessing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRBQCt-RI68L"
      },
      "source": [
        "power_usage.rename(columns={'Value (kWh)' : 'Value'}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac91-2VAFrzA"
      },
      "source": [
        "### Change date format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY5prvJWEshp"
      },
      "source": [
        "The actual StartDate of data is from 2016-06-01(yyyy-mm-dd). In the dataset the dates are not properly formated. In the following steps the StartDate column is formated properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UulfPbnFEoG3"
      },
      "source": [
        "m = power_usage.shape[0]\n",
        "power_usage_date = pd.Series(range(m), pd.period_range('2016-06-01 00:00:00', freq = '1H', periods = m))\n",
        "power_usage['StartDate'] = power_usage_date.to_frame().index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keoiJY11E0MO"
      },
      "source": [
        "m = weather.shape[0] \n",
        "weather_date = pd.Series(range(m), pd.period_range('2016-06-01', freq = '1D', periods = m))\n",
        "weather['Date'] = weather_date.to_frame().index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSw6CMcoFDjN"
      },
      "source": [
        "### Merge datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzE6MOBAFNAn"
      },
      "source": [
        "For the purpose of merging Datasets we can split the StartDate to Date and Hour columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CzXZRulFFto"
      },
      "source": [
        "for idx, date in power_usage.iterrows():\n",
        "    power_usage.loc[idx, 'Date'] = power_usage.StartDate[idx].strftime('%Y-%m-%d')\n",
        "    power_usage.loc[idx, 'Hour'] = power_usage.StartDate[idx].strftime('%H:%M')\n",
        "\n",
        "power_usage.drop(['StartDate'], inplace=True, axis = 1)\n",
        "\n",
        "\n",
        "weather_date = []\n",
        "for idx, date in weather.iterrows():\n",
        "    weather_date.append(weather.Date[idx].strftime('%Y-%m-%d'))\n",
        "weather.Date = weather_date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT0SUwN-FUjz"
      },
      "source": [
        "df_main  = power_usage.merge(weather,  on = ['Date', 'day_of_week'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8GN6xR61l6e"
      },
      "source": [
        "StopWatch.stop(\"Data preprocessing\")\n",
        "StopWatch.status(\"Data preprocessing\", True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa5-r6xnFb9o"
      },
      "source": [
        "print(f\"Shape of main table is {df_main.shape}\")\n",
        "print(f\"Shape of power_usage table is {power_usage.shape}\")\n",
        "print(f\"Shape of weather table is {weather.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h5mif2yFjn6"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtXqluSwFf5h"
      },
      "source": [
        "df_main.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSFr2QqXF7Ok"
      },
      "source": [
        "\n",
        "For better use of data , we can split Date into year, month, day columns. Since day column is already in the dataframe, we might just add year and month column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcM1ar2QFhOQ"
      },
      "source": [
        "df_main['year'] = df_main.Date.str.split('-', expand=True)[0]\n",
        "df_main['month'] = df_main.Date.str.split('-', expand=True)[1]\n",
        "df_main.drop(columns = ['Date'], inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-MOdYLbGCE6"
      },
      "source": [
        "tod = sns.lineplot('notes', 'Value', data=df_main)\n",
        "plt.xlabel('Type of day')\n",
        "plt.ylabel('Avg Value(kWh)')\n",
        "plt.title('Average power usage by type of the day')\n",
        "#tod.get_figure().savefig(\"tod.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28XSDWyPGKQt"
      },
      "source": [
        "dow = sns.barplot('day_of_week', 'Value', data=df_main)\n",
        "dow.set_xticklabels(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
        "plt.xlabel('Day of Week')\n",
        "plt.ylabel('Avg Value(kWh)')\n",
        "plt.title('Average power usage by day of the week')\n",
        "plt.xticks(rotation = 90)\n",
        "#dow.get_figure().savefig(\"dow.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b19-o_C2GNmG"
      },
      "source": [
        "sns.lineplot(x = 'month', y = 'Value', data=df_main[df_main.year.astype(int) >= 2018], hue = 'year', ci = None)\n",
        "plt.legend(labels = ['2018', '2019', '2020'])\n",
        "plt.title('Monthly power usage of three years')\n",
        "plt.ylabel('Power usage(kWh)')\n",
        "#plt.savefig(\"monthly_power.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GG5tVhrwYZb"
      },
      "source": [
        "target_corr = df_main.corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdUrlpn2zQJ2"
      },
      "source": [
        "sns.heatmap(target_corr, cmap='coolwarm')\n",
        "plt.title('Correlation matrix')\n",
        "#plt.savefig(\"corr_plot\", pad_inches=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_m1PTqJ7Pfn"
      },
      "source": [
        "We can see that features like temperature, dew and pressure has high correlation to our target feature. Also different temperatures and dew features are inter-correlated. Therefore, all the intercorrelated features except for temp_avg can be dropped during feature selection. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQx82WyG7Ze1"
      },
      "source": [
        "target_corr['Value']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqm0XdgZ7OUp"
      },
      "source": [
        "drop_cols = ['Temp_max', 'Temp_min', 'Dew_avg', 'Dew_min', 'Dew_max', 'Press_avg', 'Press_min']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3Z8M9YMGhNY"
      },
      "source": [
        "## Data Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7WkQMIaGa_b"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Pipelines\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import make_pipeline, Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
        "\n",
        "# CV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# metrics\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQtiwjan56Ho"
      },
      "source": [
        "### Split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wf8TLqsHGnze"
      },
      "source": [
        "X_train, X_test,y_train , y_test = train_test_split(df_main.drop(columns=['Value']), df_main.Value, test_size = .2, stratify= df_main.notes, random_state =42, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eiZq_sBHERs"
      },
      "source": [
        "print(f\"Train data shape is {X_train.shape}\")\n",
        "print(f\"Test data shape is {X_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNQWm5wJHFyZ"
      },
      "source": [
        "df_main.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cba4RGBF5_ca"
      },
      "source": [
        "### Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJG4OEaPHRuI"
      },
      "source": [
        "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, attribute_names):\n",
        "        self.attribute_names = attribute_names\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        return X[self.attribute_names].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EwxcsHuHR4R"
      },
      "source": [
        "cat_features = ['day_of_week', 'notes', 'Hour', 'Day', 'year', 'month']\n",
        "num_features = ['Temp_max', 'Temp_avg',\n",
        "       'Temp_min', 'Dew_max', 'Dew_avg', 'Dew_min', 'Hum_max', 'Hum_avg',\n",
        "       'Hum_min', 'Wind_max', 'Wind_avg', 'Wind_min', 'Press_max', 'Press_avg',\n",
        "       'Press_min', 'Precipit']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5X6SsNHHR_k"
      },
      "source": [
        "num_pipeline = Pipeline([\n",
        "        ('selector', DataFrameSelector(num_features)),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "# Create a pipelne for the categorical features.\n",
        "# Entries with missing values or values that don't exist in the range\n",
        "# defined above will be one hot encoded as zeroes.\n",
        "\n",
        "cat_pipeline = Pipeline([\n",
        "        ('selector', DataFrameSelector(cat_features)),\n",
        "        ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "# Union the transformed, scaled numeric and categorical features.\n",
        "data_prep_pipeline = FeatureUnion(transformer_list=[\n",
        "        (\"num_pipeline\", num_pipeline),\n",
        "        (\"cat_pipeline\", cat_pipeline),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N4WBIx6Hcyo"
      },
      "source": [
        "lin_reg = LinearRegression()\n",
        "np.random.seed(42)\n",
        "\n",
        "baseline_pipeline = Pipeline([\n",
        "    ('preperation', data_prep_pipeline),\n",
        "    ('baseline_linreg', lin_reg)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w5igRrcHc9o"
      },
      "source": [
        "cvSplits = ShuffleSplit(n_splits=10, test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWvXNJ2WKhdJ"
      },
      "source": [
        "### Baseline linear regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIyy59qTHc_c"
      },
      "source": [
        "StopWatch.start(\"Baseline Linear Regression\")\n",
        "baseline_pipeline.fit(X_train, y_train)\n",
        "np.random.seed(42)\n",
        "\n",
        "lin_scores = cross_val_score(baseline_pipeline, X_train, y_train, cv=cvSplits, scoring = 'neg_mean_squared_error')\n",
        "                               \n",
        "lin_score_train = np.sqrt(-lin_scores.mean())\n",
        "\n",
        "# Time and score test predictions\n",
        "\n",
        "lin_test_pred = baseline_pipeline.predict(X_test)\n",
        "\n",
        "lin_score_test  = np.sqrt(mean_squared_error(y_test, lin_test_pred))\n",
        "lin_r2_score = r2_score(y_test, lin_test_pred)\n",
        "\n",
        "StopWatch.stop(\"Baseline Linear Regression\")\n",
        "StopWatch.status(\"Baseline Linear Regression\", True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2A4QosqHdEX"
      },
      "source": [
        "results = pd.DataFrame(columns=[\"ExpID\", \n",
        "              \"Train RMSE\",\"Test RMSE\", \"Test R2\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRDMYjFzHpta"
      },
      "source": [
        "results.loc[0] = [\"Baseline Linear Regression\", \n",
        "                  np.round(lin_score_train,4), np.round(lin_score_test,4), np.round(lin_r2_score, 4)]\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v25ZcIT2KokQ"
      },
      "source": [
        "### Linear regression with predictors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXPyO0CyKtwu"
      },
      "source": [
        "classifiers = [\n",
        "        ('Linear Regression', LinearRegression()),\n",
        "        ('Gradient Boosting', GradientBoostingRegressor(warm_start=True, random_state=42)),\n",
        "        ('XGBoost', XGBRegressor(random_state=42)),\n",
        "        ('Light GBM', LGBMRegressor(random_state=42)),\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLBaCRhbXx9w"
      },
      "source": [
        "#GradientBoostingRegressor().get_params().keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoUMz93JNQh9"
      },
      "source": [
        "params_grid = {\n",
        "        'Linear Regression': {\n",
        "            'fit_intercept': [True, False],\n",
        "        },\n",
        "        'Gradient Boosting':  {\n",
        "            'max_depth': [10, 20, 30],\n",
        "            'max_features': [20, 50, 100],\n",
        "            'validation_fraction': [0.2],\n",
        "            'n_iter_no_change': [10],\n",
        "            'tol': [0.01],\n",
        "            'n_estimators':[20, 50, 100],\n",
        "        },\n",
        "        'XGBoost':  {\n",
        "            'max_depth': [10, 20, 30],\n",
        "            'n_estimators':[20, 50, 100],\n",
        "            'learning_rate': [0.1, 0.01, 0.001]\n",
        "        },\n",
        "        'Light GBM':  {\n",
        "            'max_depth': [10, 20, 30],\n",
        "            'num_leaves': [5, 10, 15],\n",
        "            'n_estimators':[20, 50, 100],\n",
        "            'learning_rate': [0.1, 0.01, 0.001],\n",
        "            'reg_alpha': [0.1, 0.01, 0.001],\n",
        "            'reg_lambda': [0.1, 0.01, 0.001],\n",
        "        },\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vIu8-e173Wz"
      },
      "source": [
        "#### Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUuTTMDNNQr_"
      },
      "source": [
        "dropped_df = df_main.drop(columns=drop_cols).copy()\n",
        "num_features = list(set(num_features) - set(drop_cols))\n",
        "X_train, X_test,y_train , y_test = train_test_split(dropped_df.drop(columns=['Value']), dropped_df.Value, test_size = .2, stratify= dropped_df.notes, random_state =42, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVTCUl9y-vAH"
      },
      "source": [
        "num_pipeline = Pipeline([\n",
        "        ('selector', DataFrameSelector(num_features)),\n",
        "        ('std_scaler', StandardScaler()),\n",
        "    ])\n",
        "\n",
        "# Create a pipelne for the categorical features.\n",
        "# Entries with missing values or values that don't exist in the range\n",
        "# defined above will be one hot encoded as zeroes.\n",
        "\n",
        "cat_pipeline = Pipeline([\n",
        "        ('selector', DataFrameSelector(cat_features)),\n",
        "      ('ohe', OneHotEncoder(sparse=False, handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "\n",
        "# Union the transformed, scaled numeric and categorical features.\n",
        "data_prep_pipeline = FeatureUnion(transformer_list=[\n",
        "        (\"num_pipeline\", num_pipeline),\n",
        "        (\"cat_pipeline\", cat_pipeline),\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az0LuTER8odT"
      },
      "source": [
        "i = 1\n",
        "for (name, classifier) in classifiers:\n",
        "  StopWatch.start(name)\n",
        "  parameters = params_grid[name].copy()\n",
        "\n",
        "  params = {}\n",
        "  for p in parameters.keys():\n",
        "      pipe_key = 'predictor__'+str(p)\n",
        "      params[pipe_key] = parameters[p] \n",
        "\n",
        "  pipe = Pipeline([('preperation', data_prep_pipeline),\n",
        "    ('predictor', classifier)])\n",
        "  \n",
        "  print(f\"********** Starting {name} modelling********\")\n",
        "\n",
        "  gridsearch_model = GridSearchCV(pipe, param_grid=params, cv = 5, n_jobs=-1, verbose=2)\n",
        "  gridsearch_model.fit(X_train, y_train)\n",
        "\n",
        "  best_scores = cross_val_score(gridsearch_model.best_estimator_, X_train, y_train, cv=cvSplits, scoring = 'neg_mean_squared_error')\n",
        "                               \n",
        "  best_score_train = -best_scores.mean()\n",
        "\n",
        "  y_test_pred = gridsearch_model.best_estimator_.predict(X_test)\n",
        "\n",
        "  best_score_test = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "  best_r2_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "  StopWatch.stop(name)\n",
        "  StopWatch.status(name, True)\n",
        "\n",
        "  print(f\"********** End of {name} modelling********\")\n",
        "  #best_scores_test  = baseline_pipeline.score(X_test, y_test)\n",
        "\n",
        "  results.loc[i] = [name, \n",
        "                  np.round(best_score_train,4), np.round(best_score_test,4), np.round(best_r2_test, 4)]\n",
        "\n",
        "  pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "  i += 1\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQZJkl1pA6L-"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5GxpvutXnQE"
      },
      "source": [
        "StopWatch.benchmark()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}